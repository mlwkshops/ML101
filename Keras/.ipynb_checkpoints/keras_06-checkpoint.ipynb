{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Previous: <a href = \"keras_05.ipynb\">1.5 Overfitting VS generalization</a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <center> Keras </center>\n",
    "## <center>1.6 Increasing/Decreasing number of layers</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Explanation\n",
    "There are three types of layers:\n",
    "- input layer\n",
    "- hidden layer\n",
    "- output layer\n",
    "\n",
    "One hidden layer is sufficient for the large majority of problems.\n",
    "\n",
    "There is no \"magic\" rule to calculate the number of hidden layers and nodes of Neural Network, but there are some tips and recommendations.\n",
    "\n",
    "The number of hidden nodes is based on a relationship between:\n",
    "\n",
    "- Number of input and output nodes\n",
    "- Amount of training data available\n",
    "- Complexity of the function that is trying to be learned\n",
    "- The training algorithm\n",
    "- To minimize the error and have a trained network that generalizes well, you need to pick an optimal number of hidden layers, as well as nodes in each hidden layer.\n",
    "\n",
    "- Too few nodes will lead to high error for your system as the predictive factors might be too complex for a small number of nodes to capture\n",
    "\n",
    "- Too many nodes will overfit to your training data and not generalize well\n",
    "\n",
    "<img src=\"img/hiddenlayers.png\" width=\"60%\" />"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "#previously done\n",
    "from keras.models import Sequential\n",
    "from keras.layers.core import Dense, Dropout\n",
    "from keras.optimizers import SGD, Adam, Adamax\n",
    "from keras.utils import np_utils\n",
    "from keras.utils.vis_utils import model_to_dot\n",
    "from keras.datasets import mnist\n",
    "from keras.utils import np_utils\n",
    "%matplotlib inline\n",
    "import math\n",
    "import random\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from IPython.display import SVG\n",
    "#Load MNIST\n",
    "(X_train, y_train), (X_test, y_test) = mnist.load_data()\n",
    "#Reshape\n",
    "X_train = X_train.reshape(60000, 784)\n",
    "X_test = X_test.reshape(10000, 784)\n",
    "X_train = X_train.astype('float32')\n",
    "X_test = X_test.astype('float32')\n",
    "X_train /= 255\n",
    "X_test /= 255\n",
    "Y_train = np_utils.to_categorical(y_train, 10)\n",
    "Y_test = np_utils.to_categorical(y_test, 10)\n",
    "#Split\n",
    "X_train = X_train[0:10000]\n",
    "X_test = X_test[0:1000]\n",
    "Y_train = Y_train[0:10000]\n",
    "Y_test = Y_test[0:1000]\n",
    "\n",
    "def plot_training_history(history):\n",
    "    plt.plot(history.history['acc'])\n",
    "    plt.plot(history.history['val_acc'])\n",
    "    plt.title('model accuracy')\n",
    "    plt.ylabel('accuracy')\n",
    "    plt.xlabel('epoch')\n",
    "    plt.legend(['train', 'test'], loc='upper left')\n",
    "    plt.show()\n",
    "    #loss\n",
    "    plt.plot(history.history['loss'])\n",
    "    plt.plot(history.history['val_loss'])\n",
    "    plt.title('model loss')\n",
    "    plt.ylabel('loss')\n",
    "    plt.xlabel('epoch')\n",
    "    plt.legend(['train', 'test'], loc='upper left')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 10000 samples, validate on 1000 samples\n",
      "Epoch 1/10\n",
      "10000/10000 [==============================] - 10s 1ms/step - loss: 0.0900 - acc: 0.1508 - val_loss: 0.0900 - val_acc: 0.1650\n",
      "Epoch 2/10\n",
      "10000/10000 [==============================] - 10s 1ms/step - loss: 0.0900 - acc: 0.1478 - val_loss: 0.0900 - val_acc: 0.1330\n",
      "Epoch 3/10\n",
      "10000/10000 [==============================] - 10s 1ms/step - loss: 0.0900 - acc: 0.1285 - val_loss: 0.0899 - val_acc: 0.1280\n",
      "Epoch 4/10\n",
      " 1600/10000 [===>..........................] - ETA: 7s - loss: 0.0899 - acc: 0.1188"
     ]
    }
   ],
   "source": [
    "model = Sequential()\n",
    "model.add(Dense(input_dim=28*28, units=500, activation='relu'))\n",
    "model.add(Dense(units=500, activation='relu'))\n",
    "model.add(Dense(units=500, activation='relu'))\n",
    "model.add(Dense(units=500, activation='relu'))\n",
    "model.add(Dense(units=500, activation='relu'))\n",
    "model.add(Dense(units=500, activation='relu'))\n",
    "model.add(Dense(units=500, activation='relu'))\n",
    "model.add(Dense(units=500, activation='relu'))\n",
    "model.add(Dense(units=500, activation='relu'))\n",
    "model.add(Dense(units=500, activation='relu'))\n",
    "model.add(Dense(units=500, activation='relu'))\n",
    "model.add(Dense(units=500, activation='relu'))\n",
    "model.add(Dense(units=10, activation='softmax'))\n",
    "\n",
    "model.compile(loss='mse',\n",
    "              optimizer=SGD(lr=0.1),\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "BATCH_SIZE=100\n",
    "NP_EPOCHS=10\n",
    "history = model.fit(X_train, Y_train,\n",
    "                    batch_size=BATCH_SIZE, epochs=NP_EPOCHS,\n",
    "                    verbose=1, validation_data=(X_test, Y_test))\n",
    "plot_training_history(history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "model.add(Dense(input_dim=28*28, units=500, activation='relu'))\n",
    "model.add(Dense(units=10, activation='softmax'))\n",
    "\n",
    "model.compile(loss='mse',\n",
    "              optimizer=SGD(lr=0.1),\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "BATCH_SIZE=100\n",
    "NP_EPOCHS=10\n",
    "history = model.fit(X_train, Y_train,\n",
    "                    batch_size=BATCH_SIZE, epochs=NP_EPOCHS,\n",
    "                    verbose=1, validation_data=(X_test, Y_test))\n",
    "plot_training_history(history)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task\n",
    "Name two disadvantages resulting from having to many layers."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feedback"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### Next: <a href = \"keras_07.ipynb\">1.7 Batch size and number of epochs</a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.4"
  },
  "livereveal": {
   "scroll": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
