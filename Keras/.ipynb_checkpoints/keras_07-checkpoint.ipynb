{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Previous: <a href = \"keras_06.ipynb\">1.6 Increasing/Decreasing number of layers</a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <center> Keras </center>\n",
    "## <center>1.7 Batch size and number of epochs</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Explanation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Batch size__ is the number of training instances used in one iteration. <br> <br>\n",
    " *For instance, let's say you have 1000 training samples and you want to set up batch_size equal to 100. The algorithm then takes the first 100 samples (1-100) from the training dataset and trains the network. Next it takes once again 100 samples (101-200) and trains the network again. We  keep doing this procedure until all the samples have been used for training. * <br>After each batch, the weights are adjusted.  <br>\n",
    " \n",
    "An __Epoch__ defines the number of training iterations, each sample has been through. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "#previously done\n",
    "from keras.models import Sequential\n",
    "from keras.layers.core import Dense, Dropout\n",
    "from keras.optimizers import SGD, Adam, Adamax\n",
    "from keras.utils import np_utils\n",
    "from keras.utils.vis_utils import model_to_dot\n",
    "from keras.datasets import mnist\n",
    "from keras.datasets import mnist\n",
    "from keras.utils import np_utils\n",
    "%matplotlib inline\n",
    "import math\n",
    "import random\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from IPython.display import SVG\n",
    "#Load MNIST\n",
    "(X_train, y_train), (X_test, y_test) = mnist.load_data()\n",
    "#Reshape\n",
    "X_train = X_train.reshape(60000, 784)\n",
    "X_test = X_test.reshape(10000, 784)\n",
    "X_train = X_train.astype('float32')\n",
    "X_test = X_test.astype('float32')\n",
    "X_train /= 255\n",
    "X_test /= 255\n",
    "Y_train = np_utils.to_categorical(y_train, 10)\n",
    "Y_test = np_utils.to_categorical(y_test, 10)\n",
    "#Split\n",
    "X_train = X_train[0:10000]\n",
    "X_test = X_test[0:1000]\n",
    "Y_train = Y_train[0:10000]\n",
    "Y_test = Y_test[0:1000]\n",
    "\n",
    "def plot_training_history(history):\n",
    "    plt.plot(history.history['acc'])\n",
    "    plt.plot(history.history['val_acc'])\n",
    "    plt.title('model accuracy')\n",
    "    plt.ylabel('accuracy')\n",
    "    plt.xlabel('epoch')\n",
    "    plt.legend(['train', 'test'], loc='upper left')\n",
    "    plt.show()\n",
    "    #loss\n",
    "    plt.plot(history.history['loss'])\n",
    "    plt.plot(history.history['val_loss'])\n",
    "    plt.title('model loss')\n",
    "    plt.ylabel('loss')\n",
    "    plt.xlabel('epoch')\n",
    "    plt.legend(['train', 'test'], loc='upper left')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 10000 samples, validate on 1000 samples\n",
      "Epoch 1/3\n",
      " 2671/10000 [=======>......................] - ETA: 58s - loss: 0.0385 - acc: 0.7301"
     ]
    }
   ],
   "source": [
    "model = Sequential()\n",
    "model.add(Dense(input_dim=28*28, units=500, activation='relu'))\n",
    "model.add(Dense(units=500, activation='relu'))\n",
    "model.add(Dense(units=10, activation='softmax'))\n",
    "\n",
    "model.compile(loss='mse',\n",
    "              optimizer=SGD(lr=0.1),\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "BATCH_SIZE=1\n",
    "NP_EPOCHS=3\n",
    "history = model.fit(X_train, Y_train,\n",
    "                    batch_size=BATCH_SIZE, epochs=NP_EPOCHS,\n",
    "                    verbose=1, validation_data=(X_test, Y_test))\n",
    "plot_training_history(history)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task\n",
    "Find an equation that computes the number of updates performed by the optimizer using batch size 'bs' and epoch size 'es' as parameters. The number of samples is located in numSamples."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feedback"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Next: <a href = \"keras_08.ipynb\">1.8 Loss function</a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.4"
  },
  "livereveal": {
   "scroll": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
