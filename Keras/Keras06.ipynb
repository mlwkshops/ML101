{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <center> Keras </center>\n",
    "## <center>1.5.1 Activation functions</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Activation functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Activation functions are used to transform the output of a layer. The activation functions can be linear or non-linear. \n",
    "Activation functions are an extremely important feature of the artificial neural networks. They basically decide whether a neuron should be activated or not. Whether the information that the neuron is receiving is relevant for the given information or should it be ignored. They add non-linearity to the basic dot-product and multiplication of layers.<br>\n",
    "Keras supports the following activation functions by default but one can easily create a custom activation function as well: \n",
    "- softmax\n",
    "- elu\n",
    "- selu\n",
    "- softplus\n",
    "- softsign\n",
    "- relu\n",
    "- tanh\n",
    "- sigmoid\n",
    "- hard_sigmoid\n",
    "- linear\n",
    "\n",
    "<br>\n",
    "Some common activation functions are shown below:\n",
    "<img src=\"img/activation_functions.JPG\" width = \"80%\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Code"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us run the code from the previous section first."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Importing the MNIST dataset\n",
    "from keras.datasets import mnist\n",
    "(train_images, train_labels), (test_images, test_labels) = mnist.load_data()\n",
    "\n",
    "# Processing the input data\n",
    "train_images = train_images.reshape((60000, 28 * 28))\n",
    "train_images = train_images.astype('float32') / 255\n",
    "\n",
    "test_images = test_images.reshape((10000, 28 * 28))\n",
    "test_images = test_images.astype('float32') / 255\n",
    "\n",
    "# Processing the output data\n",
    "from keras.utils import to_categorical\n",
    "\n",
    "train_labels = to_categorical(train_labels)\n",
    "test_labels = to_categorical(test_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us now examine the code related to building a network. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Build a network\n",
    "from keras import models\n",
    "from keras import layers\n",
    "\n",
    "network = models.Sequential()\n",
    "network.add(layers.Dense(units=512, activation='relu', input_shape=(28 * 28,)))\n",
    "network.add(layers.Dense(units=10, activation='softmax'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we are using `relu` as the activation function between input and first hidden layer and `softmax` at the output layer."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Best practice\n",
    "At the output layer, it is recommended to use the softmax function for binary and multiclass classification, because the output values sum up to 1. This is useful for calculating probabilities via binary cross-entropy and cross-entropy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "def plot_training_history(history):\n",
    "    plt.plot(history.history['acc'])\n",
    "    plt.plot(history.history['val_acc'])\n",
    "    plt.title('model accuracy')\n",
    "    plt.ylabel('accuracy')\n",
    "    plt.xlabel('epoch')\n",
    "    plt.legend(['train', 'test'], loc='upper left')\n",
    "    plt.show()\n",
    "    #loss\n",
    "    plt.plot(history.history['loss'])\n",
    "    plt.plot(history.history['val_loss'])\n",
    "    plt.title('model loss')\n",
    "    plt.ylabel('loss')\n",
    "    plt.xlabel('epoch')\n",
    "    plt.legend(['train', 'test'], loc='upper left')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Compile the network\n",
    "network.compile(optimizer='rmsprop',\n",
    "                loss='categorical_crossentropy',\n",
    "                metrics=['accuracy'])\n",
    "\n",
    "# Train the network\n",
    "history = network.fit(train_images, train_labels, epochs=5, batch_size=128, \n",
    "                      verbose=1, validation_data=(test_images, test_labels))\n",
    "\n",
    "# Plot the training results\n",
    "plot_training_history(history)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task\n",
    "Change the activation functions and check how the network is performing. Note down the results:<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Summary\n",
    "In this section we learned about the activation functions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Feedback\n",
    "<a href = \"http://goto/ml101_doc/Keras06\">Feedback: Activation functions</a> <br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Navigation\n",
    "\n",
    "<div>\n",
    "<span> <h3 style=\"display:inline\">&lt;&lt; Prev: <a href = \"keras05.ipynb\">Build a network</a></h3> </span>\n",
    "<span style=\"float: right\"><h3 style=\"display:inline\">Next: <a href = \"keras07.ipynb\">Units</a> &gt;&gt; </h3></span>\n",
    "</div>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
