{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <center> Keras </center>\n",
    "## <center>1.6.2 Optimizers</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Optimizers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "An optimizer determines how the network will be updated based on the loss function. It implements a specific variant of stochastic gradient descent.\n",
    "\n",
    "The optimizer function tries to minimize the loss.\n",
    "<img src=\"img/structure_optimizer.PNG\" width=\"60%\" />\n",
    "Some optimizer functions:\n",
    "- SGD\n",
    "- RMSprop\n",
    "- Adagrad\n",
    "- Adadelta\n",
    "- Adam\n",
    "- Adamax\n",
    "- Nadam\n",
    "\n",
    "### Follow opposite direction of gradient to achieve less loss. \n",
    "<img src=\"img/optimizer2.png\" width=\"40%\" />\n",
    "\n",
    "### The algorithm could get stuck in a local minimum -> global minimum never reached.\n",
    "<img src=\"img/optimizer1.png\" width=\"40%\" />\n",
    "\n",
    "### Other functions can avoid this issue.\n",
    "<img src=\"img/optimizer.gif\" width=\"50%\" />\n",
    "\n",
    "### Evolution of optimizers\n",
    "<img src=\"img/optimizers.png\" width=\"70%\" />\n",
    "\n",
    "## Best practice\n",
    "You can use Adam as a default."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Code"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us run the code from the previous section first."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing the MNIST dataset\n",
    "from keras.datasets import mnist\n",
    "(train_images, train_labels), (test_images, test_labels) = mnist.load_data()\n",
    "\n",
    "# Processing the input data\n",
    "train_images = train_images.reshape((60000, 28 * 28))\n",
    "train_images = train_images.astype('float32') / 255\n",
    "\n",
    "test_images = test_images.reshape((10000, 28 * 28))\n",
    "test_images = test_images.astype('float32') / 255\n",
    "\n",
    "# Processing the output data\n",
    "from keras.utils import to_categorical\n",
    "\n",
    "train_labels = to_categorical(train_labels)\n",
    "test_labels = to_categorical(test_labels)\n",
    "\n",
    "# Build a network\n",
    "from keras import models\n",
    "from keras import layers\n",
    "\n",
    "network = models.Sequential()\n",
    "network.add(layers.Dense(units=512, activation='relu', input_shape=(28 * 28,)))\n",
    "network.add(layers.Dense(units=10, activation='softmax'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us now examine the code related to compiling a network. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Compile the network\n",
    "network.compile(optimizer='rmsprop',\n",
    "                loss='categorical_crossentropy',\n",
    "                metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we are using `rmsprop` as the optimizer and `categorical_crossentropy` as the loss function. The performance metric is `accuracy`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "def plot_training_history(history):\n",
    "    plt.plot(history.history['acc'])\n",
    "    plt.plot(history.history['val_acc'])\n",
    "    plt.title('model accuracy')\n",
    "    plt.ylabel('accuracy')\n",
    "    plt.xlabel('epoch')\n",
    "    plt.legend(['train', 'test'], loc='upper left')\n",
    "    plt.show()\n",
    "    #loss\n",
    "    plt.plot(history.history['loss'])\n",
    "    plt.plot(history.history['val_loss'])\n",
    "    plt.title('model loss')\n",
    "    plt.ylabel('loss')\n",
    "    plt.xlabel('epoch')\n",
    "    plt.legend(['train', 'test'], loc='upper left')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train the network\n",
    "history = network.fit(train_images, train_labels, epochs=5, batch_size=128, \n",
    "                      verbose=1, validation_data=(test_images, test_labels))\n",
    "\n",
    "# Plot the training results\n",
    "plot_training_history(history)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task\n",
    "Change the optimizer and note down the results."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Summary\n",
    "In this section we learned about the optimizers."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Feedback\n",
    "<a href = \"http://goto/ml101_doc/Keras10\">Feedback: Optimizers</a> <br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Navigation\n",
    "\n",
    "<div>\n",
    "<span> <h3 style=\"display:inline\">&lt;&lt; Prev: <a href = \"keras09.ipynb\">Loss functions</a></h3> </span>\n",
    "<span style=\"float: right\"><h3 style=\"display:inline\">Next: <a href = \"keras11.ipynb\">Train a network</a> &gt;&gt; </h3></span>\n",
    "</div>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
