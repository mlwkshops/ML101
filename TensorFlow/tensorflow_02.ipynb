{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Previous: <a href = \"tensorflow_01.ipynb\">2.1 Structure</a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <center> TensorFlow </center>\n",
    "## <center>2.2 Weights</center>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Explanation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The specification of what a layer does to its input data is stored in the layer’s\n",
    "__weights__, which in essence are a bunch of numbers. In technical terms, you would say\n",
    "that the transformation implemented by a layer is \"parametrized\" by its weights.\n",
    "In fact, weights are also sometimes called the \"parameters\" of a layer. <br>\n",
    "In this context, \"learning\" will mean finding a set of values for the weights of all layers in a network, such that the\n",
    "network will correctly map your example inputs to their associated targets. But here’s the\n",
    "thing: a deep neural network can contain tens of millions of parameters. Finding the\n",
    "correct value for all of them may seem like a daunting task, especially since modifying\n",
    "the value of one parameter will affect the behavior of all others!\n",
    "<br>\n",
    "<img src=\"img/weights.jpg\" width=\"50%\" /><br>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "mnist = input_data.read_data_sets('MNIST_data', one_hot=True)\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import random as ran\n",
    "\n",
    "def TRAIN_SIZE(num):\n",
    "    #print ('Total Training Images in Dataset = ' + str(mnist.train.images.shape))\n",
    "    #print ('--------------------------------------------------')\n",
    "    x_train = mnist.train.images[:num,:]\n",
    "    #print ('x_train Examples Loaded = ' + str(x_train.shape))\n",
    "    y_train = mnist.train.labels[:num,:]\n",
    "    #print ('y_train Examples Loaded = ' + str(y_train.shape))\n",
    "    #print('')\n",
    "    return x_train, y_train\n",
    "\n",
    "def TEST_SIZE(num):\n",
    "    #print ('Total Test Examples in Dataset = ' + str(mnist.test.images.shape))\n",
    "    #print ('--------------------------------------------------')\n",
    "    x_test = mnist.test.images[:num,:]\n",
    "    #print ('x_test Examples Loaded = ' + str(x_test.shape))\n",
    "    y_test = mnist.test.labels[:num,:]\n",
    "    #print ('y_test Examples Loaded = ' + str(y_test.shape))\n",
    "    return x_test, y_test\n",
    "\n",
    "def display_digit(num):\n",
    "    print(y_train[num])\n",
    "    label = y_train[num].argmax(axis=0)\n",
    "    image = x_train[num].reshape([28,28])\n",
    "    plt.title('Example: %d  Label: %d' % (num, label))\n",
    "    plt.imshow(image, cmap=plt.get_cmap('gray_r'))\n",
    "    plt.show()\n",
    "\n",
    "def display_mult_flat(start, stop):\n",
    "    images = x_train[start].reshape([1,784])\n",
    "    for i in range(start+1,stop):\n",
    "        images = np.concatenate((images, x_train[i].reshape([1,784])))\n",
    "    plt.imshow(images, cmap=plt.get_cmap('gray_r'))\n",
    "    plt.show()\n",
    "    \n",
    "x_train, y_train = TRAIN_SIZE(55000)\n",
    "\n",
    "import tensorflow as tf\n",
    "sess = tf.Session()\n",
    "x = tf.placeholder(tf.float32, shape=[None, 784])\n",
    "y_ = tf.placeholder(tf.float32, shape=[None, 10])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Example"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the next step, we will define the weights W and bias b. These two values are the grunt workers of the classifier. They will be the only values we will need to calculate our prediction after the classifier is trained.\n",
    "\n",
    "We will first set our weight and bias values to zeros because TensorFlow will optimize these values later. Notice how our W is a collection of 784 values for each of the 10 classes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "W = tf.Variable(tf.zeros([784,10]))\n",
    "b = tf.Variable(tf.zeros([10]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can think of the weights as 10 cheat sheets for each number, which is similar to how a teacher uses a cheat sheet transparency to grade a multiple choice exam.\n",
    "<br> <br>The bias is a little beyond the scope of this workshop, but you can think of it as a special relationship with the weights that influences the final answer.\n",
    "<br> <br>\n",
    "We will now define y, which is our classifier function.  We make our prediction by multiplying each flattened digit by our weight and then adding our bias. __Matmul__ is the function for multiplying matrices."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "y = tf.nn.softmax(tf.matmul(x,W) + b)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## in a graph:\n",
    "<img src=\"https://www.tensorflow.org/versions/master/images/softmax-regression-scalargraph.png\" width = \"60%\"/>\n",
    "\n",
    "## in a vector equation:\n",
    "<img src=\"https://www.tensorflow.org/versions/master/images/softmax-regression-vectorequation.png\" width = \"60%\"/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train, y_train = TRAIN_SIZE(3)\n",
    "sess.run(tf.global_variables_initializer())\n",
    "sess.run(y, feed_dict={x: x_train})\n",
    "cross_entropy = tf.reduce_mean(-tf.reduce_sum(y_ * tf.log(y), reduction_indices=[1]))\n",
    "x_train, y_train = TRAIN_SIZE(5500)\n",
    "x_test, y_test = TEST_SIZE(10000)\n",
    "LEARNING_RATE = 0.1\n",
    "TRAIN_STEPS = 2500\n",
    "init = tf.global_variables_initializer()\n",
    "sess.run(init)\n",
    "training = tf.train.GradientDescentOptimizer(LEARNING_RATE).minimize(cross_entropy)\n",
    "correct_prediction = tf.equal(tf.argmax(y,1), tf.argmax(y_,1))\n",
    "accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n",
    "for i in range(TRAIN_STEPS+1):\n",
    "    sess.run(training, feed_dict={x: x_train, y_: y_train})\n",
    "    if i%100 == 0:\n",
    "        print('Training Step:' + str(i) + '  Accuracy =  ' + str(sess.run(accuracy, feed_dict={x: x_test, y_: y_test})) + '  Loss = ' + str(sess.run(cross_entropy, {x: x_train, y_: y_train})))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Weights\n",
    "<img src=\"img/Weights.png\" width = \"50%\"/><br>\n",
    "In this image, if the red pixel is filled, the digit being classified is very likely to be a one, thus has positive weight. <br>\n",
    "On the right side, if the blue pixel is filled, the digit being classified is very likely not a zero, thus has negative weight.  \n",
    "<br>\n",
    "After we have calculated our weight cheatsheet, we can create a graph with the following code:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(10):\n",
    "    plt.subplot(2, 5, i+1)\n",
    "    weight = sess.run(W)[:,i]\n",
    "    plt.title(i)\n",
    "    plt.imshow(weight.reshape([28,28]), cmap=plt.get_cmap('seismic'))\n",
    "    frame1 = plt.gca()\n",
    "    frame1.axes.get_xaxis().set_visible(False)\n",
    "    frame1.axes.get_yaxis().set_visible(False)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is a visualization of our weights from 0-9, which is the most important aspect of our classifier. The bulk of the work of machine learning is figuring out what the optimal weights are. Once they are calculated, you have the “cheat sheet” and can easily find answers. \n",
    "\n",
    "This is why neural networks can be easily used within mobile devices, as  the model, once trained, doesn’t take up that much room to store or computing power to calculate. \n",
    "\n",
    "Our classifier makes its prediction by comparing how similar or different the digit is to the red and blue. <br>You can think the darker the red, the better of a hit; white as neutral; and blue as misses.\n",
    "\n",
    "So, now that we have our cheat sheet, let’s load one example and apply our classifier to that one example."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train, y_train = TRAIN_SIZE(1)\n",
    "display_digit(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let’s look at our predictor y. \n",
    "This gives us a (1x10) matrix with each column containing one probability."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "answer = sess.run(y, feed_dict={x: x_train})\n",
    "print(answer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is not very useful for us. So, we use the __argmax__ function to return the position of the highest value and that gives us our prediction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "answer.argmax()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's create a function to make predictions on a random digit in this data set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def display_compare(num):\n",
    "    # THIS WILL LOAD ONE TRAINING EXAMPLE\n",
    "    x_train = mnist.train.images[num,:].reshape(1,784)\n",
    "    y_train = mnist.train.labels[num,:]\n",
    "    # THIS GETS OUR LABEL AS A INTEGER\n",
    "    label = y_train.argmax()\n",
    "    # THIS GETS OUR PREDICTION AS A INTEGER\n",
    "    prediction = sess.run(y, feed_dict={x: x_train}).argmax()\n",
    "    plt.title('Prediction: %d Label: %d' % (prediction, label))\n",
    "    plt.imshow(x_train.reshape([28,28]), cmap=plt.get_cmap('gray_r'))\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let's try out the function. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display_compare(ran.randint(0, 55000))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display_compare(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here, the digit has been predicted incorrectly.  \n",
    "\n",
    "Why do you think it got it wrong?\n",
    "\n",
    "Notice what happens to the visualizations of the weights when you use 1-10 training examples. It becomes clear that using too little data makes it very hard to generalize. Here is an animation showing how the weights change as you increase your training size. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"img/a.gif\" width = \"65%\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feedback"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
