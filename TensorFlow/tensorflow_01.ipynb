{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Previous:   <a href=\"../Keras/keras_01.ipynb\">Check out Keras 1.1 MNIST</a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <center> TensorFlow </center>\n",
    "## <center>2.1 Structure</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Explanation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Example"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Importing the MNIST dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "mnist = input_data.read_data_sets('MNIST_data', one_hot=True)\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import random as ran"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Functions are defined, which are assigning the amount of training and test data, we will load from the data set.<br>\n",
    "We also define functions for resizing and displaying the data.<br>\n",
    "It's not important to look deeply into those functions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def TRAIN_SIZE(num):\n",
    "    print ('Total Training Images in Dataset = ' + str(mnist.train.images.shape))\n",
    "    print ('--------------------------------------------------')\n",
    "    x_train = mnist.train.images[:num,:]\n",
    "    print ('x_train Examples Loaded = ' + str(x_train.shape))\n",
    "    y_train = mnist.train.labels[:num,:]\n",
    "    print ('y_train Examples Loaded = ' + str(y_train.shape))\n",
    "    print('')\n",
    "    return x_train, y_train\n",
    "\n",
    "def TEST_SIZE(num):\n",
    "    print ('Total Test Examples in Dataset = ' + str(mnist.test.images.shape))\n",
    "    print ('--------------------------------------------------')\n",
    "    x_test = mnist.test.images[:num,:]\n",
    "    print ('x_test Examples Loaded = ' + str(x_test.shape))\n",
    "    y_test = mnist.test.labels[:num,:]\n",
    "    print ('y_test Examples Loaded = ' + str(y_test.shape))\n",
    "    return x_test, y_test\n",
    "\n",
    "def display_digit(num):\n",
    "    print(y_train[num])\n",
    "    label = y_train[num].argmax(axis=0)\n",
    "    image = x_train[num].reshape([28,28])\n",
    "    plt.title('Example: %d  Label: %d' % (num, label))\n",
    "    plt.imshow(image, cmap=plt.get_cmap('gray_r'))\n",
    "    plt.show()\n",
    "\n",
    "def display_mult_flat(start, stop):\n",
    "    images = x_train[start].reshape([1,784])\n",
    "    for i in range(start+1,stop):\n",
    "        images = np.concatenate((images, x_train[i].reshape([1,784])))\n",
    "    plt.imshow(images, cmap=plt.get_cmap('gray_r'))\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, we define variables with how many training and test examples we would like to load. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train, y_train = TRAIN_SIZE(55000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So, what does this mean? In our data set, there are 55,000 examples of handwritten digits from zero to nine.\n",
    "Each example is a 28x28 pixel image flattened in an array with 784 values representing each pixel’s intensity. \n",
    "The examples need to be flattened for TensorFlow to make sense of the digits linearly. \n",
    "This shows that in x_train we have loaded 55,000 examples each with 784 pixels. \n",
    "Our x_train variable is a 55,000 row and 784 column matrix.\n",
    "\n",
    "The y_train data is the associated labels for all the x_train examples. \n",
    "Rather than storing the label as an integer, it is stored as a 1x10 binary array with the one representing the digit. \n",
    "This is also known as one-hot encoding. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display_digit(ran.randint(0, x_train.shape[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here is what multiple training examples look like to the classifier in their flattened form. \n",
    "Of course, instead of pixels, our classifier sees values from zero to one representing pixel intensity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display_mult_flat(0,500)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Until this point, we actually have not been using TensorFlow at all. <br>\n",
    "The next step is importing TensorFlow and defining our session. \n",
    "TensorFlow, in a sense, creates a graph which you later feed with data and run in a session."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "sess = tf.Session()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we define a placeholder. <br>\n",
    "A placeholder is a variable used to feed data into. \n",
    "The only requirement is that in order to feed data into this variable, we need to match its shape and type exactly. <br><br>Here, we define our x placeholder as the variable to feed our x_train data into."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "x = tf.placeholder(tf.float32, shape=[None, 784])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When assigning __None__ to our placeholder, it means the placeholder can be fed as many examples as you want to give it.<br><br>\n",
    "We then define y_, which will be used to feed y_train into. This will be used later so we can compare the ground truths to our predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "y_ = tf.placeholder(tf.float32, shape=[None, 10])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will have a closer look into this part in the <a href=\"url\">following notebook</a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "W = tf.Variable(tf.zeros([784,10]))\n",
    "b = tf.Variable(tf.zeros([10]))\n",
    "y = tf.nn.softmax(tf.matmul(x,W) + b)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You cannot just print a TensorFlow graph object to get its values, you must run an appropriate session in which you feed it data. \n",
    "In order to run a function in our session, we first must initialize the variables in our session.<br>\n",
    "Let’s feed our classifier three examples and see what it predicts. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train, y_train = TRAIN_SIZE(3)\n",
    "sess.run(tf.global_variables_initializer())\n",
    "#If using TensorFlow prior to 0.12 use:\n",
    "#sess.run(tf.initialize_all_variables())\n",
    "print(sess.run(y, feed_dict={x: x_train}))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So, here we can see our prediction for our first three training examples. Of course, our classifier knows nothing at this point, so it outputs an equal 10% probability of our training examples for each possible class.\n",
    "\n",
    "How did TensorFlow know the probabilities?<br>\n",
    "It learned the probabilities by calculating the softmax of our results. The Softmax function takes a set of values and forces their sum to equal one, which will give probabilities for each value. Any softmax value will always be greater than zero and less than one. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we create our __cross_entropy function__, also known as loss or cost function. It measures how good (or bad) we are classifying. <br>The higher the cost, the higher the level of inaccuracy. It calculates accuracy by comparing the true values from y_train to the results of our prediction y for each example. Our goal is to minimize the loss."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "cross_entropy = tf.reduce_mean(-tf.reduce_sum(y_ * tf.log(y), reduction_indices=[1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below is where we can now assign custom variables for training. Any value that is in all caps below is designed to be changed and messed with.<br>In fact, just try it out! <br>First, use these values, then later notice what happens when you use too few training examples or too high or low of a learning rate.\n",
    "<br>\n",
    "If you set TRAIN_SIZE to a large number, be prepared to wait for a while. At any point, you can re run all the code starting from here and try different values:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train, y_train = TRAIN_SIZE(5500)\n",
    "x_test, y_test = TEST_SIZE(10000)\n",
    "LEARNING_RATE = 0.1\n",
    "TRAIN_STEPS = 2500"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now initialize all variables so that they can be used by our TensorFlow graph."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "init = tf.global_variables_initializer()\n",
    "sess.run(init)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we need to train our classifier using gradient descent. We first define our training method and some variables for measuring our accuracy. The variable training will perform the gradient descent optimizer with a chosen LEARNING_RATE in order to try to minimize our loss."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "training = tf.train.GradientDescentOptimizer(LEARNING_RATE).minimize(cross_entropy)\n",
    "correct_prediction = tf.equal(tf.argmax(y,1), tf.argmax(y_,1))\n",
    "accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we’ll define a loop that repeats __TRAIN_STEPS__ times.<br>\n",
    "For each loop, it runs training, feeding in values from x_train and y_train using feed_dict.<br>\n",
    "\n",
    "In order to calculate accuracy, it will run accuracy to classify the __unseen__ data in x_test by comparing its y and y_test. \n",
    "It is vitally important that our test data was unseen and not used for training data. If a teacher were to give students a practice exam and use that same exam for the final exam, you would have a very biased measure of students’ knowledge."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(TRAIN_STEPS+1):\n",
    "    sess.run(training, feed_dict={x: x_train, y_: y_train})\n",
    "    if i%100 == 0:\n",
    "        print('Training Step:' + str(i) + '  Accuracy =  ' + str(sess.run(accuracy, feed_dict={x: x_test, y_: y_test})) + '  Loss = ' + str(sess.run(cross_entropy, {x: x_train, y_: y_train})))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Reference: <br>\n",
    "https://www.oreilly.com/learning/not-another-mnist-tutorial-with-tensorflow <br>\n",
    "https://www.tensorflow.org/versions/master/images/softmax-regression-scalargraph.png <br>\n",
    "https://www.tensorflow.org/versions/master/images/softmax-regression-vectorequation.png"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feedback"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Next: <a href = \"tensorflow_02.ipynb\">2.2 Weights</a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
